{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "275de153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a656929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_files={}\n",
    "seg_files={}\n",
    "\n",
    "seg_img_root='Resized_seg_files'\n",
    "mri_img_root = 'Resized_MRI_Volumes'\n",
    "\n",
    "\n",
    "#creating list of files for DataGenerator\n",
    "for root, dirs, files in os.walk(seg_img_root):\n",
    "    for name in files:\n",
    "        indx = name.find('.nii')\n",
    "        key = name[indx-5:indx]\n",
    "        file_path = os.path.join(root, name)\n",
    "        seg_files[key] = file_path\n",
    "\n",
    "for root, dirs, files in os.walk(mri_img_root):\n",
    "    for name in files:\n",
    "        indx = name.find('.nii')\n",
    "        key = name[indx-5:indx]\n",
    "        file_path = os.path.join(root, name)\n",
    "        mri_files[key] = file_path \n",
    "        \n",
    "        \n",
    "        \n",
    "#check that every segmentation file (ground truth) has corresponding X data\n",
    "for k in seg_files.keys():\n",
    "    if k not in mri_files.keys():\n",
    "        print('Not found in mri files:', k)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea673aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_list = [k for k in seg_files.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4e91239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, samples_list, y_dict, x_dict, batch_size=16, dim=(80,120,120), n_channels=4, shuffle=True):\n",
    "     \n",
    "        self.samples_list=samples_list\n",
    "        self.y_dict=y_dict\n",
    "        self.x_dict=x_dict\n",
    "        self.batch_size=batch_size\n",
    "        self.dim=dim\n",
    "        self.n_channels=n_channels\n",
    "        self.shuffle=shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        #Take all batches in each iteration'\n",
    "        return int(np.floor(len(self.samples_list) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Get next batch'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # single file\n",
    "        samples_list_temp = [samples_list[k] for k in indexes]\n",
    "\n",
    "        # Set of X_train and y_train\n",
    "        X, y = self.__data_generation(samples_list_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        #Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.samples_list))\n",
    "        #shuffle data if shuffle is true\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "\n",
    "    def __data_generation(self, samples_list_temp):\n",
    "        #Generates data containing batch_size samples'\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, 80,120,120,1), dtype=np.float32)\n",
    "        \n",
    "        # Generate data\n",
    "        for i, file_key in enumerate(samples_list_temp):\n",
    "            # Store sample\n",
    "            X[i,] = nibabel.load(self.x_dict[file_key]).get_fdata()\n",
    "            \n",
    "            # Store target segmentation\n",
    "            y[i,] = nibabel.load(self.y_dict[file_key]).get_fdata()\n",
    "\n",
    "        return X, y\n",
    "\n",
    "        \n",
    "# 75-20-5 train-validation-test split of data \n",
    "\n",
    "#FIXME\n",
    "#adjusted to partial sample list\n",
    "remain_samples, test_samples, _ , _ = train_test_split(samples_list, samples_list, test_size=0.05, random_state=42)\n",
    "\n",
    "train_samples, val_samples, _ , _ = train_test_split(remain_samples, remain_samples, test_size=0.21, random_state=42)\n",
    "\n",
    "#build data generators\n",
    "\n",
    "img_data_gen_args = {'batch_size':4,\n",
    "                         'dim':(80,120, 120), \n",
    "                         'n_channels':4,\n",
    "                         'shuffle':True}\n",
    "\n",
    "train_data_generator = TrainDataGenerator(samples_list = train_samples,\n",
    "                                          y_dict = seg_files,\n",
    "                                          x_dict = mri_files,\n",
    "                                          **img_data_gen_args)\n",
    "\n",
    "val_data_generator = TrainDataGenerator(samples_list = test_samples,\n",
    "                                          y_dict = seg_files,\n",
    "                                          x_dict = mri_files,\n",
    "                                          **img_data_gen_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00e00e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def IoU_wrapper(num_classes):\n",
    "    def IoU_coef(y_true, y_pred) -> float:\n",
    "        '''\n",
    "        Computes Intersection-Over-Union also known as Jaccard Index\n",
    "        For multiclass task computes IoU individually for each class\n",
    "            and then returns average. Class labels should begin with 0\n",
    "\n",
    "        Inputs:\n",
    "        ground_truth: a numpy array of the correct labels\n",
    "        y_preds: a numpy array of the predicted labels\n",
    "        num_classes: an int for the number of classes\n",
    "\n",
    "        Output:\n",
    "        a float that is the average calculated IoU\n",
    "        '''\n",
    "\n",
    "        y_true = tf.math.round(y_true)\n",
    "        y_pred = tf.math.round(y_pred)\n",
    "        iou_array = tf.TensorArray(tf.float32, size=num_classes, clear_after_read=False)\n",
    "\n",
    "        for c in range(num_classes):\n",
    "            \n",
    "            y_true_vals = tf.cast(K.flatten(tf.math.equal(y_true, tf.constant(c, tf.float32))), tf.float32)\n",
    "            y_pred_vals = tf.cast(K.flatten(tf.math.equal(y_pred, tf.constant(c, tf.float32))), tf.float32)\n",
    "            intersection = K.sum(y_true_vals * y_pred_vals)\n",
    "            union =  K.sum(y_true_vals) + K.sum(y_pred_vals) - intersection\n",
    "\n",
    "            iou_array = iou_array.write(c, intersection/union)\n",
    "        \n",
    "        return K.mean(iou_array.stack())\n",
    "    return IoU_coef\n",
    "\n",
    "\n",
    "def dice_wrapper(num_classes):\n",
    "    def dice_coef(y_true, y_pred) -> float:\n",
    "        '''\n",
    "        Computes dice coefficient\n",
    "        For multiclass task computes dice individually for each class\n",
    "            and then returns average. Class labels should begin with 0\n",
    "\n",
    "        Inputs:\n",
    "        ground_truth: a numpy array of the correct labels\n",
    "        y_preds: a numpy array of the predicted labels\n",
    "        num_classes: an int for the number of classes\n",
    "\n",
    "        Output:\n",
    "        [a float that is the average dice]\n",
    "        '''\n",
    "        y_true = tf.math.round(y_true)\n",
    "        y_pred = tf.math.round(y_pred)\n",
    "        dice_array = tf.TensorArray(tf.float32, size=num_classes, clear_after_read=False)\n",
    "\n",
    "        for c in range(num_classes):\n",
    "            y_true_vals = tf.cast(K.flatten(tf.math.equal(y_true, tf.constant(c, tf.float32))), tf.float32)\n",
    "            y_pred_vals = tf.cast(K.flatten(tf.math.equal(y_pred, tf.constant(c, tf.float32))), tf.float32)\n",
    "            intersection = K.sum(y_true_vals * y_pred_vals)\n",
    "\n",
    "            dice_array = dice_array.write(c, (2*intersection)/(K.sum(y_true_vals) + K.sum(y_pred_vals)))\n",
    "\n",
    "        \n",
    "        return K.mean(dice_array.stack())\n",
    "    return dice_coef\n",
    "\n",
    "\n",
    "def binary_IoU_coef(y_true, y_pred) -> float:\n",
    "        '''\n",
    "        Computes Intersection-Over-Union also known as Jaccard Index\n",
    "        For binary segmentation\n",
    "\n",
    "        Inputs:\n",
    "        ground_truth: a numpy array of the correct labels\n",
    "        y_preds: a numpy array of the predicted labels\n",
    "        num_classes: an int for the number of classes\n",
    "\n",
    "        Output:\n",
    "        a float of calculated IoU\n",
    "        '''\n",
    "\n",
    "        y_true_vals = K.flatten(y_true)\n",
    "        y_pred_vals = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_vals * y_pred_vals)\n",
    "        union =  K.sum(y_true_vals) + K.sum(y_pred_vals) - intersection\n",
    "\n",
    "    \n",
    "        return intersection/union\n",
    "    \n",
    "def binary_dice_coef(y_true, y_pred) -> float:\n",
    "        '''\n",
    "        Computes dice coefficient\n",
    "        For binary class segmentation\n",
    "\n",
    "        Inputs:\n",
    "        ground_truth: a numpy array of the correct labels\n",
    "        y_preds: a numpy array of the predicted labels\n",
    "        num_classes: an int for the number of classes\n",
    "\n",
    "        Output:\n",
    "        a float of dice coefficient]\n",
    "        '''\n",
    "\n",
    "        y_true_vals = K.flatten(y_true)\n",
    "        y_pred_vals = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_vals * y_pred_vals)\n",
    "\n",
    "        dice = (2*intersection)/(K.sum(y_true_vals) + K.sum(y_pred_vals))\n",
    "\n",
    "\n",
    "        return dice\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "        return (1/binary_dice_coef(y_true, y_pred)) - 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f30ac5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.33333334>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [1,0,0]\n",
    "y_pred = [1,1,1]\n",
    "\n",
    "iou_array = tf.TensorArray(tf.float32, size=1, clear_after_read=False)\n",
    "\n",
    "y_true_vals = tf.cast(K.flatten(tf.math.equal(y_true, tf.constant(1))), tf.float32)\n",
    "y_pred_vals = tf.cast(K.flatten(tf.math.equal(y_pred, tf.constant(1))), tf.float32)\n",
    "intersection = K.sum(y_true_vals * y_pred_vals)\n",
    "union =  K.sum(y_true_vals) + K.sum(y_pred_vals) - intersection\n",
    "\n",
    "iou_array = iou_array.write(0, intersection/union)\n",
    "\n",
    "K.mean(iou_array.stack())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc9ef132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"3D_semantic_segmentation\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-09 11:48:07.255901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 11:48:07.619244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 11:48:07.620919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 11:48:07.648314: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-09 11:48:07.663246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 11:48:07.664928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 11:48:07.666474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 11:48:11.193610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 11:48:11.194567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 11:48:11.195475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 11:48:11.204319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7195 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 80, 120, 12  0           []                               \n",
      "                                0, 4)]                                                            \n",
      "                                                                                                  \n",
      " conv3d (Conv3D)                (None, 80, 120, 120  1744        ['input_1[0][0]']                \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 80, 120, 120  64         ['conv3d[0][0]']                 \n",
      " alization)                     , 16)                                                             \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 80, 120, 120  0           ['batch_normalization[0][0]']    \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " conv3d_1 (Conv3D)              (None, 80, 120, 120  6928        ['activation[0][0]']             \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 80, 120, 120  64         ['conv3d_1[0][0]']               \n",
      " rmalization)                   , 16)                                                             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 80, 120, 120  0           ['batch_normalization_1[0][0]']  \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " max_pooling3d (MaxPooling3D)   (None, 40, 60, 60,   0           ['activation_1[0][0]']           \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " conv3d_2 (Conv3D)              (None, 40, 60, 60,   13856       ['max_pooling3d[0][0]']          \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 40, 60, 60,   128        ['conv3d_2[0][0]']               \n",
      " rmalization)                   32)                                                               \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 40, 60, 60,   0           ['batch_normalization_2[0][0]']  \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_3 (Conv3D)              (None, 40, 60, 60,   27680       ['activation_2[0][0]']           \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 40, 60, 60,   128        ['conv3d_3[0][0]']               \n",
      " rmalization)                   32)                                                               \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 40, 60, 60,   0           ['batch_normalization_3[0][0]']  \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " max_pooling3d_1 (MaxPooling3D)  (None, 20, 30, 30,   0          ['activation_3[0][0]']           \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_4 (Conv3D)              (None, 20, 30, 30,   55360       ['max_pooling3d_1[0][0]']        \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 20, 30, 30,   256        ['conv3d_4[0][0]']               \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 20, 30, 30,   0           ['batch_normalization_4[0][0]']  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_5 (Conv3D)              (None, 20, 30, 30,   110656      ['activation_4[0][0]']           \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 20, 30, 30,   256        ['conv3d_5[0][0]']               \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 20, 30, 30,   0           ['batch_normalization_5[0][0]']  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " max_pooling3d_2 (MaxPooling3D)  (None, 10, 15, 15,   0          ['activation_5[0][0]']           \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_6 (Conv3D)              (None, 10, 15, 15,   110656      ['max_pooling3d_2[0][0]']        \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 10, 15, 15,   256        ['conv3d_6[0][0]']               \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 10, 15, 15,   0           ['batch_normalization_6[0][0]']  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_7 (Conv3D)              (None, 10, 15, 15,   110656      ['activation_6[0][0]']           \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 10, 15, 15,   256        ['conv3d_7[0][0]']               \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 10, 15, 15,   0           ['batch_normalization_7[0][0]']  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " up_sampling3d (UpSampling3D)   (None, 20, 30, 30,   0           ['activation_7[0][0]']           \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 20, 30, 30,   0           ['activation_5[0][0]',           \n",
      "                                128)                              'up_sampling3d[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_8 (Conv3D)              (None, 20, 30, 30,   110624      ['concatenate[0][0]']            \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 20, 30, 30,   128        ['conv3d_8[0][0]']               \n",
      " rmalization)                   32)                                                               \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 20, 30, 30,   0           ['batch_normalization_8[0][0]']  \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_9 (Conv3D)              (None, 20, 30, 30,   27680       ['activation_8[0][0]']           \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 20, 30, 30,   128        ['conv3d_9[0][0]']               \n",
      " rmalization)                   32)                                                               \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 20, 30, 30,   0           ['batch_normalization_9[0][0]']  \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " up_sampling3d_1 (UpSampling3D)  (None, 40, 60, 60,   0          ['activation_9[0][0]']           \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 40, 60, 60,   0           ['activation_3[0][0]',           \n",
      "                                64)                               'up_sampling3d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv3d_10 (Conv3D)             (None, 40, 60, 60,   27664       ['concatenate_1[0][0]']          \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 40, 60, 60,   64         ['conv3d_10[0][0]']              \n",
      " ormalization)                  16)                                                               \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 40, 60, 60,   0           ['batch_normalization_10[0][0]'] \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " conv3d_11 (Conv3D)             (None, 40, 60, 60,   6928        ['activation_10[0][0]']          \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 40, 60, 60,   64         ['conv3d_11[0][0]']              \n",
      " ormalization)                  16)                                                               \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 40, 60, 60,   0           ['batch_normalization_11[0][0]'] \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " up_sampling3d_2 (UpSampling3D)  (None, 80, 120, 120  0          ['activation_11[0][0]']          \n",
      "                                , 16)                                                             \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 80, 120, 120  0           ['activation_1[0][0]',           \n",
      "                                , 32)                             'up_sampling3d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv3d_12 (Conv3D)             (None, 80, 120, 120  33          ['concatenate_2[0][0]']          \n",
      "                                , 1)                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 612,257\n",
      "Trainable params: 611,361\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, concatenate, BatchNormalization, Dense, Dropout, Flatten \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.layers import Activation, UpSampling3D, ZeroPadding3D\n",
    "\n",
    "#free up RAM\n",
    "keras.backend.clear_session()\n",
    "\n",
    "def conv_block(input_layer, num_filters):\n",
    "    x = Conv3D(filters=num_filters, kernel_size=3, strides=1, padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv3D(filters=num_filters, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def encoder_block(input_layer, num_filters):\n",
    "    x = conv_block(input_layer, num_filters)\n",
    "    out = MaxPooling3D((2,2,2))(x)\n",
    "    \n",
    "    return out, x\n",
    "\n",
    "def decoder_block(input_layer, conc_layer, num_filters):\n",
    "    x = conv_block(input_layer, num_filters)\n",
    "    x = UpSampling3D(size=2)(x)\n",
    "    out = concatenate([conc_layer, x])\n",
    "    \n",
    "    return out\n",
    "    \n",
    "def build_classifier(input_shape):\n",
    "\n",
    "    input_layer = Input(input_shape)\n",
    "    \n",
    "    #encoder layers\n",
    "    c1, u1 = encoder_block(input_layer, 16)\n",
    "    c2, u2 = encoder_block(c1,32)\n",
    "    c3, u3 = encoder_block(c2, 64)\n",
    "    \n",
    "    #decoder layers\n",
    "    c6 = decoder_block(c3, u3, 64)\n",
    "    c7 = decoder_block(c6, u2, 32)\n",
    "    c8 = decoder_block(c7, u1, 16)\n",
    "    \n",
    "    segmentation_layer = Conv3D(filters=1, kernel_size=1, activation='sigmoid', padding='same')(c8)\n",
    "    \n",
    "    model = Model(input_layer, segmentation_layer, name='3D_semantic_segmentation')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#free up RAM\n",
    "keras.backend.clear_session()\n",
    "\n",
    "#build model\n",
    "input_shape = (80, 120, 120, 4)\n",
    "model = build_classifier(input_shape)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cf75382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/908 [..............................] - ETA: 36:01 - loss: 1.6446 - acc: 0.9960 - IoU_coef: 0.8803 - dice_coef: 0.9323 - binary_IoU_coef: 0.2331 - binary_dice_coef: 0.3781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-09 11:54:03.043563: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 18432000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/908 [==>...........................] - ETA: 5:46 - loss: 2.1783 - acc: 0.9930 - IoU_coef: 0.7808 - dice_coef: 0.8483 - binary_IoU_coef: 0.2714 - binary_dice_coef: 0.4112"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mdice_loss, optimizer\u001b[38;5;241m=\u001b[39mAdam(), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m, iou_metric, dice_metric, \n\u001b[1;32m      6\u001b[0m                                                                      binary_IoU_coef, binary_dice_coef])\n\u001b[1;32m      7\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 8\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_generator\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#testing customs metrics\n",
    "iou_metric = IoU_wrapper(2)\n",
    "dice_metric = dice_wrapper(2)\n",
    "\n",
    "model.compile(loss=dice_loss, optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
    "                                                                     binary_IoU_coef, binary_dice_coef])\n",
    "epochs=1\n",
    "history = model.fit(train_data_generator , \n",
    "                              validation_data=val_data_generator,\n",
    "                             epochs=epochs,\n",
    "                             verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef32c407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 120, 120, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92cc4d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "227/227 [==============================] - 450s 2s/step - loss: 0.4164 - acc: 0.9941 - IoU_coef: 0.7930 - dice_coef: 0.8669 - binary_IoU_coef: 0.5747 - binary_dice_coef: 0.7231 - val_loss: 1.1812 - val_acc: 0.9945 - val_IoU_coef: 0.8093 - val_dice_coef: 0.8703 - val_binary_IoU_coef: 0.5943 - val_binary_dice_coef: 0.7226\n",
      "Epoch 2/30\n",
      "227/227 [==============================] - 445s 2s/step - loss: 0.2829 - acc: 0.9954 - IoU_coef: 0.8309 - dice_coef: 0.8971 - binary_IoU_coef: 0.6515 - binary_dice_coef: 0.7856 - val_loss: 528.7045 - val_acc: 0.9952 - val_IoU_coef: 0.8090 - val_dice_coef: 0.8716 - val_binary_IoU_coef: 0.6030 - val_binary_dice_coef: 0.7317\n",
      "Epoch 3/30\n",
      "227/227 [==============================] - 445s 2s/step - loss: 0.2416 - acc: 0.9959 - IoU_coef: 0.8471 - dice_coef: 0.9087 - binary_IoU_coef: 0.6847 - binary_dice_coef: 0.8100 - val_loss: 47.3841 - val_acc: 0.9952 - val_IoU_coef: 0.8067 - val_dice_coef: 0.8643 - val_binary_IoU_coef: 0.6099 - val_binary_dice_coef: 0.7263\n",
      "Epoch 4/30\n",
      "227/227 [==============================] - 443s 2s/step - loss: 0.2225 - acc: 0.9961 - IoU_coef: 0.8552 - dice_coef: 0.9144 - binary_IoU_coef: 0.7024 - binary_dice_coef: 0.8225 - val_loss: 16.1403 - val_acc: 0.9950 - val_IoU_coef: 0.7990 - val_dice_coef: 0.8541 - val_binary_IoU_coef: 0.5932 - val_binary_dice_coef: 0.7051\n",
      "Epoch 5/30\n",
      "227/227 [==============================] - 441s 2s/step - loss: 0.2037 - acc: 0.9964 - IoU_coef: 0.8633 - dice_coef: 0.9199 - binary_IoU_coef: 0.7195 - binary_dice_coef: 0.8345 - val_loss: 0.2916 - val_acc: 0.9965 - val_IoU_coef: 0.8553 - val_dice_coef: 0.9094 - val_binary_IoU_coef: 0.7007 - val_binary_dice_coef: 0.8116\n",
      "Epoch 6/30\n",
      "227/227 [==============================] - 447s 2s/step - loss: 0.1776 - acc: 0.9968 - IoU_coef: 0.8769 - dice_coef: 0.9290 - binary_IoU_coef: 0.7465 - binary_dice_coef: 0.8526 - val_loss: 0.2684 - val_acc: 0.9965 - val_IoU_coef: 0.8671 - val_dice_coef: 0.9174 - val_binary_IoU_coef: 0.7252 - val_binary_dice_coef: 0.8286\n",
      "Epoch 7/30\n",
      "227/227 [==============================] - 443s 2s/step - loss: 0.1804 - acc: 0.9968 - IoU_coef: 0.8758 - dice_coef: 0.9280 - binary_IoU_coef: 0.7455 - binary_dice_coef: 0.8516 - val_loss: 0.2999 - val_acc: 0.9959 - val_IoU_coef: 0.8675 - val_dice_coef: 0.9175 - val_binary_IoU_coef: 0.7265 - val_binary_dice_coef: 0.8287\n",
      "Epoch 8/30\n",
      "227/227 [==============================] - 443s 2s/step - loss: 0.1651 - acc: 0.9970 - IoU_coef: 0.8831 - dice_coef: 0.9329 - binary_IoU_coef: 0.7603 - binary_dice_coef: 0.8617 - val_loss: 0.1859 - val_acc: 0.9974 - val_IoU_coef: 0.8867 - val_dice_coef: 0.9328 - val_binary_IoU_coef: 0.7654 - val_binary_dice_coef: 0.8602\n",
      "Epoch 9/30\n",
      "227/227 [==============================] - 438s 2s/step - loss: 0.1643 - acc: 0.9970 - IoU_coef: 0.8845 - dice_coef: 0.9337 - binary_IoU_coef: 0.7629 - binary_dice_coef: 0.8631 - val_loss: 0.2165 - val_acc: 0.9969 - val_IoU_coef: 0.8771 - val_dice_coef: 0.9252 - val_binary_IoU_coef: 0.7449 - val_binary_dice_coef: 0.8439\n",
      "Epoch 10/30\n",
      "227/227 [==============================] - 442s 2s/step - loss: 0.1438 - acc: 0.9973 - IoU_coef: 0.8945 - dice_coef: 0.9404 - binary_IoU_coef: 0.7837 - binary_dice_coef: 0.8771 - val_loss: 836.5490 - val_acc: 0.9964 - val_IoU_coef: 0.8390 - val_dice_coef: 0.8912 - val_binary_IoU_coef: 0.6753 - val_binary_dice_coef: 0.7803\n",
      "Epoch 11/30\n",
      "227/227 [==============================] - 437s 2s/step - loss: 0.1365 - acc: 0.9974 - IoU_coef: 0.8980 - dice_coef: 0.9427 - binary_IoU_coef: 0.7907 - binary_dice_coef: 0.8818 - val_loss: 6738.4189 - val_acc: 0.9974 - val_IoU_coef: 0.8808 - val_dice_coef: 0.9250 - val_binary_IoU_coef: 0.7559 - val_binary_dice_coef: 0.8463\n",
      "Epoch 12/30\n",
      "227/227 [==============================] - 439s 2s/step - loss: 0.1295 - acc: 0.9975 - IoU_coef: 0.9014 - dice_coef: 0.9450 - binary_IoU_coef: 0.7979 - binary_dice_coef: 0.8866 - val_loss: 0.1670 - val_acc: 0.9972 - val_IoU_coef: 0.8900 - val_dice_coef: 0.9357 - val_binary_IoU_coef: 0.7736 - val_binary_dice_coef: 0.8669\n",
      "Epoch 13/30\n",
      "227/227 [==============================] - 443s 2s/step - loss: 0.1334 - acc: 0.9974 - IoU_coef: 0.8994 - dice_coef: 0.9436 - binary_IoU_coef: 0.7942 - binary_dice_coef: 0.8840 - val_loss: 0.1785 - val_acc: 0.9968 - val_IoU_coef: 0.8825 - val_dice_coef: 0.9310 - val_binary_IoU_coef: 0.7600 - val_binary_dice_coef: 0.8585\n",
      "Epoch 14/30\n",
      "227/227 [==============================] - 441s 2s/step - loss: 0.1335 - acc: 0.9974 - IoU_coef: 0.9007 - dice_coef: 0.9441 - binary_IoU_coef: 0.7971 - binary_dice_coef: 0.8853 - val_loss: 0.1909 - val_acc: 0.9971 - val_IoU_coef: 0.8832 - val_dice_coef: 0.9301 - val_binary_IoU_coef: 0.7632 - val_binary_dice_coef: 0.8579\n",
      "Epoch 15/30\n",
      "227/227 [==============================] - 440s 2s/step - loss: 0.1254 - acc: 0.9976 - IoU_coef: 0.9041 - dice_coef: 0.9465 - binary_IoU_coef: 0.8040 - binary_dice_coef: 0.8902 - val_loss: 0.4454 - val_acc: 0.9971 - val_IoU_coef: 0.8657 - val_dice_coef: 0.9126 - val_binary_IoU_coef: 0.7295 - val_binary_dice_coef: 0.8239\n",
      "Epoch 16/30\n",
      "227/227 [==============================] - 450s 2s/step - loss: 0.1153 - acc: 0.9977 - IoU_coef: 0.9102 - dice_coef: 0.9503 - binary_IoU_coef: 0.8162 - binary_dice_coef: 0.8979 - val_loss: 0.2066 - val_acc: 0.9975 - val_IoU_coef: 0.8963 - val_dice_coef: 0.9381 - val_binary_IoU_coef: 0.7881 - val_binary_dice_coef: 0.8732\n",
      "Epoch 17/30\n",
      "227/227 [==============================] - 440s 2s/step - loss: 0.1129 - acc: 0.9978 - IoU_coef: 0.9119 - dice_coef: 0.9513 - binary_IoU_coef: 0.8193 - binary_dice_coef: 0.8997 - val_loss: 3.4110 - val_acc: 0.9976 - val_IoU_coef: 0.8936 - val_dice_coef: 0.9350 - val_binary_IoU_coef: 0.7819 - val_binary_dice_coef: 0.8665\n",
      "Epoch 18/30\n",
      "227/227 [==============================] - 437s 2s/step - loss: 0.1213 - acc: 0.9977 - IoU_coef: 0.9068 - dice_coef: 0.9481 - binary_IoU_coef: 0.8096 - binary_dice_coef: 0.8935 - val_loss: 0.9540 - val_acc: 0.9973 - val_IoU_coef: 0.8754 - val_dice_coef: 0.9181 - val_binary_IoU_coef: 0.7500 - val_binary_dice_coef: 0.8356\n",
      "Epoch 19/30\n",
      "227/227 [==============================] - 437s 2s/step - loss: 0.1077 - acc: 0.9979 - IoU_coef: 0.9150 - dice_coef: 0.9533 - binary_IoU_coef: 0.8256 - binary_dice_coef: 0.9037 - val_loss: 11755.1670 - val_acc: 0.9977 - val_IoU_coef: 0.8879 - val_dice_coef: 0.9271 - val_binary_IoU_coef: 0.7718 - val_binary_dice_coef: 0.8516\n",
      "Epoch 20/30\n",
      "227/227 [==============================] - 439s 2s/step - loss: 0.1107 - acc: 0.9978 - IoU_coef: 0.9130 - dice_coef: 0.9520 - binary_IoU_coef: 0.8223 - binary_dice_coef: 0.9016 - val_loss: 0.4328 - val_acc: 0.9976 - val_IoU_coef: 0.8978 - val_dice_coef: 0.9374 - val_binary_IoU_coef: 0.7927 - val_binary_dice_coef: 0.8728\n",
      "Epoch 21/30\n",
      "227/227 [==============================] - 439s 2s/step - loss: 0.1077 - acc: 0.9979 - IoU_coef: 0.9151 - dice_coef: 0.9533 - binary_IoU_coef: 0.8265 - binary_dice_coef: 0.9041 - val_loss: 3.5550 - val_acc: 0.9974 - val_IoU_coef: 0.8899 - val_dice_coef: 0.9310 - val_binary_IoU_coef: 0.7764 - val_binary_dice_coef: 0.8597\n",
      "Epoch 22/30\n",
      "227/227 [==============================] - 441s 2s/step - loss: 0.1070 - acc: 0.9979 - IoU_coef: 0.9152 - dice_coef: 0.9534 - binary_IoU_coef: 0.8269 - binary_dice_coef: 0.9044 - val_loss: 0.1237 - val_acc: 0.9979 - val_IoU_coef: 0.9109 - val_dice_coef: 0.9494 - val_binary_IoU_coef: 0.8176 - val_binary_dice_coef: 0.8960\n",
      "Epoch 23/30\n",
      "227/227 [==============================] - 442s 2s/step - loss: 0.1079 - acc: 0.9979 - IoU_coef: 0.9149 - dice_coef: 0.9531 - binary_IoU_coef: 0.8262 - binary_dice_coef: 0.9039 - val_loss: 0.1422 - val_acc: 0.9977 - val_IoU_coef: 0.9069 - val_dice_coef: 0.9458 - val_binary_IoU_coef: 0.8108 - val_binary_dice_coef: 0.8896\n",
      "Epoch 24/30\n",
      "227/227 [==============================] - 444s 2s/step - loss: 0.1039 - acc: 0.9979 - IoU_coef: 0.9170 - dice_coef: 0.9545 - binary_IoU_coef: 0.8305 - binary_dice_coef: 0.9067 - val_loss: 0.1950 - val_acc: 0.9976 - val_IoU_coef: 0.9011 - val_dice_coef: 0.9404 - val_binary_IoU_coef: 0.7989 - val_binary_dice_coef: 0.8787\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - 433s 2s/step - loss: 0.1052 - acc: 0.9979 - IoU_coef: 0.9168 - dice_coef: 0.9542 - binary_IoU_coef: 0.8305 - binary_dice_coef: 0.9063 - val_loss: 0.1945 - val_acc: 0.9977 - val_IoU_coef: 0.8976 - val_dice_coef: 0.9388 - val_binary_IoU_coef: 0.7919 - val_binary_dice_coef: 0.8754\n",
      "Epoch 26/30\n",
      "227/227 [==============================] - 437s 2s/step - loss: 0.1095 - acc: 0.9978 - IoU_coef: 0.9136 - dice_coef: 0.9523 - binary_IoU_coef: 0.8243 - binary_dice_coef: 0.9027 - val_loss: 0.1364 - val_acc: 0.9979 - val_IoU_coef: 0.9094 - val_dice_coef: 0.9474 - val_binary_IoU_coef: 0.8151 - val_binary_dice_coef: 0.8924\n",
      "Epoch 27/30\n",
      "227/227 [==============================] - 442s 2s/step - loss: 0.1081 - acc: 0.9978 - IoU_coef: 0.9142 - dice_coef: 0.9527 - binary_IoU_coef: 0.8256 - binary_dice_coef: 0.9036 - val_loss: 0.1447 - val_acc: 0.9974 - val_IoU_coef: 0.9066 - val_dice_coef: 0.9458 - val_binary_IoU_coef: 0.8100 - val_binary_dice_coef: 0.8895\n",
      "Epoch 28/30\n",
      "173/227 [=====================>........] - ETA: 1:35 - loss: 0.1178 - acc: 0.9977 - IoU_coef: 0.9099 - dice_coef: 0.9497 - binary_IoU_coef: 0.8174 - binary_dice_coef: 0.8977"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mdice_loss, optimizer\u001b[38;5;241m=\u001b[39mAdam(), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m, iou_metric, dice_metric, \n\u001b[1;32m      6\u001b[0m                                                                      binary_IoU_coef, binary_dice_coef])\n\u001b[1;32m      8\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m\n\u001b[0;32m----> 9\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_generator\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/venv-tf2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('resized_MRI_segmentation_dice_loss.h5', save_weights_only=True)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='binary_dice_coef', patience=10, mode='max')\n",
    "\n",
    "callbacks = [checkpoint, early_stopping]\n",
    "model.compile(loss=dice_loss, optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
    "                                                                     binary_IoU_coef, binary_dice_coef])\n",
    "\n",
    "epochs=30\n",
    "history = model.fit(train_data_generator , \n",
    "                              validation_data=val_data_generator,\n",
    "                             epochs=epochs,\n",
    "                             verbose=1,\n",
    "                             callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c34a68a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "578e38a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainHistoryDict\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_pi:\n\u001b[0;32m----> 2\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory, file_pi)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "with open('trainHistoryDict', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2825d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import numpy as np\n",
      "import nibabel\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from tensorflow import keras\n",
      "mri_files={}\n",
      "seg_files={}\n",
      "\n",
      "seg_img_root='Resized_seg_files'\n",
      "mri_img_root = 'Resized_MRI_Volumes'\n",
      "\n",
      "\n",
      "#creating list of files for DataGenerator\n",
      "for root, dirs, files in os.walk(seg_img_root):\n",
      "    for name in files:\n",
      "        indx = name.find('.nii')\n",
      "        key = name[indx-5:indx]\n",
      "        file_path = os.path.join(root, name)\n",
      "        seg_files[key] = file_path\n",
      "\n",
      "for root, dirs, files in os.walk(mri_img_root):\n",
      "    for name in files:\n",
      "        indx = name.find('.nii')\n",
      "        key = name[indx-5:indx]\n",
      "        file_path = os.path.join(root, name)\n",
      "        mri_files[key] = file_path \n",
      "        \n",
      "        \n",
      "        \n",
      "#check that every segmentation file (ground truth) has corresponding X data\n",
      "for k in seg_files.keys():\n",
      "    if k not in mri_files.keys():\n",
      "        print('Not found in mri files:', k)\n",
      "samples_list = [k for k in seg_files.keys()]\n",
      "class TrainDataGenerator(keras.utils.Sequence):\n",
      "\n",
      "    def __init__(self, samples_list, y_dict, x_dict, batch_size=16, dim=(80,120,120), n_channels=4, shuffle=True):\n",
      "     \n",
      "        self.samples_list=samples_list\n",
      "        self.y_dict=y_dict\n",
      "        self.x_dict=x_dict\n",
      "        self.batch_size=batch_size\n",
      "        self.dim=dim\n",
      "        self.n_channels=n_channels\n",
      "        self.shuffle=shuffle\n",
      "        self.on_epoch_end()\n",
      "\n",
      "    def __len__(self):\n",
      "        #Take all batches in each iteration'\n",
      "        return int(np.floor(len(self.samples_list) / self.batch_size))\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        'Get next batch'\n",
      "        # Generate indexes of the batch\n",
      "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
      "\n",
      "        # single file\n",
      "        samples_list_temp = [samples_list[k] for k in indexes]\n",
      "\n",
      "        # Set of X_train and y_train\n",
      "        X, y = self.__data_generation(samples_list_temp)\n",
      "\n",
      "        return X, y\n",
      "\n",
      "    def on_epoch_end(self):\n",
      "        #Updates indexes after each epoch'\n",
      "        self.indexes = np.arange(len(self.samples_list))\n",
      "        #shuffle data if shuffle is true\n",
      "        if self.shuffle == True:\n",
      "            np.random.shuffle(self.indexes)\n",
      "            \n",
      "\n",
      "    def __data_generation(self, samples_list_temp):\n",
      "        #Generates data containing batch_size samples'\n",
      "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
      "        y = np.empty((self.batch_size, 80,120,120,1), dtype=np.uint8)\n",
      "        \n",
      "        # Generate data\n",
      "        for i, file_key in enumerate(samples_list_temp):\n",
      "            # Store sample\n",
      "            X[i,] = nibabel.load(self.x_dict[file_key]).get_fdata()\n",
      "            \n",
      "            # Store target segmentation\n",
      "            y[i,] = nibabel.load(self.y_dict[file_key]).get_fdata().astype('uint8')\n",
      "\n",
      "        return X, y\n",
      "\n",
      "        \n",
      "# 75-20-5 train-validation-test split of data \n",
      "\n",
      "#FIXME\n",
      "#adjusted to partial sample list\n",
      "remain_samples, test_samples, _ , _ = train_test_split(samples_list, samples_list, test_size=0.05, random_state=42)\n",
      "\n",
      "train_samples, val_samples, _ , _ = train_test_split(remain_samples, remain_samples, test_size=0.21, random_state=42)\n",
      "\n",
      "#build data generators\n",
      "\n",
      "img_data_gen_args = {'batch_size':1,\n",
      "                         'dim':(80,120, 120), \n",
      "                         'n_channels':4,\n",
      "                         'shuffle':True}\n",
      "\n",
      "train_data_generator = TrainDataGenerator(samples_list = train_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "val_data_generator = TrainDataGenerator(samples_list = test_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "import keras.backend as K\n",
      "\n",
      "def IoU_wrapper(num_classes):\n",
      "    def IoU_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes Intersection-Over-Union also known as Jaccard Index\n",
      "        For multiclass task computes IoU individually for each class\n",
      "            and then returns average. Class labels should begin with 0\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float that is the average calculated IoU\n",
      "        '''\n",
      "\n",
      "        y_true = tf.math.round(y_true)\n",
      "        y_pred = tf.math.round(y_pred)\n",
      "        iou_array = tf.TensorArray(tf.float32, size=num_classes, clear_after_read=False)\n",
      "\n",
      "        for c in range(num_classes):\n",
      "            \n",
      "            y_true_vals = tf.cast(K.flatten(tf.math.equal(y_true, tf.constant(c, tf.float32))), tf.float32)\n",
      "            y_pred_vals = tf.cast(K.flatten(tf.math.equal(y_pred, tf.constant(c, tf.float32))), tf.float32)\n",
      "            intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "            union =  K.sum(y_true_vals) + K.sum(y_pred_vals) - intersection\n",
      "\n",
      "            iou_array = iou_array.write(c, intersection/union)\n",
      "        \n",
      "        return K.mean(iou_array.stack())\n",
      "    return IoU_coef\n",
      "\n",
      "\n",
      "def dice_wrapper(num_classes):\n",
      "    def dice_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes dice coefficient\n",
      "        For multiclass task computes dice individually for each class\n",
      "            and then returns average. Class labels should begin with 0\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        [a float that is the average dice]\n",
      "        '''\n",
      "        y_true = tf.math.round(y_true)\n",
      "        y_pred = tf.math.round(y_pred)\n",
      "        dice_array = tf.TensorArray(tf.float32, size=num_classes, clear_after_read=False)\n",
      "\n",
      "        for c in range(num_classes):\n",
      "            y_true_vals = tf.cast(K.flatten(tf.math.equal(y_true, tf.constant(c, tf.float32))), tf.float32)\n",
      "            y_pred_vals = tf.cast(K.flatten(tf.math.equal(y_pred, tf.constant(c, tf.float32))), tf.float32)\n",
      "            intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "\n",
      "            dice_array = dice_array.write(c, (2*intersection)/(K.sum(y_true_vals) + K.sum(y_pred_vals)))\n",
      "\n",
      "        \n",
      "        return K.mean(dice_array.stack())\n",
      "    return dice_coef\n",
      "\n",
      "\n",
      "def binary_IoU_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes Intersection-Over-Union also known as Jaccard Index\n",
      "        For binary segmentation\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float of calculated IoU\n",
      "        '''\n",
      "\n",
      "        y_true_vals = K.flatten(y_true)\n",
      "        y_pred_vals = K.flatten(y_pred)\n",
      "        intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "        union =  K.sum(y_true_vals) + K.sum(y_pred_vals) - intersection\n",
      "\n",
      "    \n",
      "        return intersection/union\n",
      "    \n",
      "def binary_dice_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes dice coefficient\n",
      "        For binary class segmentation\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float of dice coefficient]\n",
      "        '''\n",
      "\n",
      "        y_true_vals = K.flatten(y_true)\n",
      "        y_pred_vals = K.flatten(y_pred)\n",
      "        intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "\n",
      "        dice = (2*intersection)/(K.sum(y_true_vals) + K.sum(y_pred_vals))\n",
      "\n",
      "\n",
      "        return dice\n",
      "\n",
      "def dice_loss(dice):\n",
      "        return (1/dice) - 1\n",
      "\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras.models import Model\n",
      "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, concatenate, BatchNormalization, Dense, Dropout, Flatten \n",
      "from tensorflow.keras.optimizers import Adam\n",
      "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
      "from tensorflow.keras.layers import Activation, UpSampling3D, ZeroPadding3D\n",
      "\n",
      "#free up RAM\n",
      "keras.backend.clear_session()\n",
      "\n",
      "def conv_block(input_layer, num_filters):\n",
      "    x = Conv3D(filters=num_filters, kernel_size=3, strides=1, padding='same')(input_layer)\n",
      "    x = BatchNormalization()(x)\n",
      "    x = Activation('relu')(x)\n",
      "    \n",
      "    x = Conv3D(filters=num_filters, kernel_size=3, strides=1, padding='same')(x)\n",
      "    x = BatchNormalization()(x)\n",
      "    x = Activation('relu')(x)\n",
      "    \n",
      "    return x\n",
      "\n",
      "def encoder_block(input_layer, num_filters):\n",
      "    x = conv_block(input_layer, num_filters)\n",
      "    out = MaxPooling3D((2,2,2))(x)\n",
      "    \n",
      "    return out, x\n",
      "\n",
      "def decoder_block(input_layer, conc_layer, num_filters):\n",
      "    x = conv_block(input_layer, num_filters)\n",
      "    x = UpSampling3D(size=2)(x)\n",
      "    out = concatenate([conc_layer, x])\n",
      "    \n",
      "    return out\n",
      "    \n",
      "def build_classifier(input_shape):\n",
      "\n",
      "    input_layer = Input(input_shape)\n",
      "    \n",
      "    #encoder layers\n",
      "    c1, u1 = encoder_block(input_layer, 16)\n",
      "    c2, u2 = encoder_block(c1,32)\n",
      "    c3, u3 = encoder_block(c2, 64)\n",
      "    \n",
      "    #decoder layers\n",
      "    c6 = decoder_block(c3, u3, 64)\n",
      "    c7 = decoder_block(c6, u2, 32)\n",
      "    c8 = decoder_block(c7, u1, 16)\n",
      "    \n",
      "    segmentation_layer = Conv3D(filters=1, kernel_size=1, activation='sigmoid', padding='same')(c8)\n",
      "    \n",
      "    model = Model(input_layer, segmentation_layer, name='3D_semantic_segmentation')\n",
      "\n",
      "    return model\n",
      "\n",
      "\n",
      "#free up RAM\n",
      "keras.backend.clear_session()\n",
      "\n",
      "#build model\n",
      "input_shape = (80, 120, 120, 4)\n",
      "model = build_classifier(input_shape)\n",
      "print(model.summary())\n",
      "#testing customs metrics\n",
      "iou_metric = IoU_wrapper(2)\n",
      "dice_metric = dice_wrapper(2)\n",
      "\n",
      "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
      "                                                                     binary_IoU_coef, binary_dice_coef])\n",
      "epochs=1\n",
      "history = model.fit(train_data_generator , \n",
      "                              validation_data=val_data_generator,\n",
      "                             epochs=epochs,\n",
      "                             verbose=1,)\n",
      "import keras.backend as K\n",
      "\n",
      "def IoU_wrapper(num_classes):\n",
      "    def IoU_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes Intersection-Over-Union also known as Jaccard Index\n",
      "        For multiclass task computes IoU individually for each class\n",
      "            and then returns average. Class labels should begin with 0\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float that is the average calculated IoU\n",
      "        '''\n",
      "\n",
      "        y_true = tf.math.round(y_true)\n",
      "        y_pred = tf.math.round(y_pred)\n",
      "        iou_array = tf.TensorArray(tf.float32, size=num_classes, clear_after_read=False)\n",
      "\n",
      "        for c in range(num_classes):\n",
      "            \n",
      "            y_true_vals = tf.cast(K.flatten(tf.math.equal(y_true, tf.constant(c, tf.float32))), tf.float32)\n",
      "            y_pred_vals = tf.cast(K.flatten(tf.math.equal(y_pred, tf.constant(c, tf.float32))), tf.float32)\n",
      "            intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "            union =  K.sum(y_true_vals) + K.sum(y_pred_vals) - intersection\n",
      "\n",
      "            iou_array = iou_array.write(c, intersection/union)\n",
      "        \n",
      "        return K.mean(iou_array.stack())\n",
      "    return IoU_coef\n",
      "\n",
      "\n",
      "def dice_wrapper(num_classes):\n",
      "    def dice_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes dice coefficient\n",
      "        For multiclass task computes dice individually for each class\n",
      "            and then returns average. Class labels should begin with 0\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        [a float that is the average dice]\n",
      "        '''\n",
      "        y_true = tf.math.round(y_true)\n",
      "        y_pred = tf.math.round(y_pred)\n",
      "        dice_array = tf.TensorArray(tf.float32, size=num_classes, clear_after_read=False)\n",
      "\n",
      "        for c in range(num_classes):\n",
      "            y_true_vals = tf.cast(K.flatten(tf.math.equal(y_true, tf.constant(c, tf.float32))), tf.float32)\n",
      "            y_pred_vals = tf.cast(K.flatten(tf.math.equal(y_pred, tf.constant(c, tf.float32))), tf.float32)\n",
      "            intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "\n",
      "            dice_array = dice_array.write(c, (2*intersection)/(K.sum(y_true_vals) + K.sum(y_pred_vals)))\n",
      "\n",
      "        \n",
      "        return K.mean(dice_array.stack())\n",
      "    return dice_coef\n",
      "\n",
      "\n",
      "def binary_IoU_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes Intersection-Over-Union also known as Jaccard Index\n",
      "        For binary segmentation\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float of calculated IoU\n",
      "        '''\n",
      "\n",
      "        y_true_vals = K.flatten(y_true)\n",
      "        y_pred_vals = K.flatten(y_pred)\n",
      "        intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "        union =  K.sum(y_true_vals) + K.sum(y_pred_vals) - intersection\n",
      "\n",
      "    \n",
      "        return intersection/union\n",
      "    \n",
      "def binary_dice_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes dice coefficient\n",
      "        For binary class segmentation\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float of dice coefficient]\n",
      "        '''\n",
      "\n",
      "        y_true_vals = K.flatten(y_true)\n",
      "        y_pred_vals = K.flatten(y_pred)\n",
      "        intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "\n",
      "        dice = (2*intersection)/(K.sum(y_true_vals) + K.sum(y_pred_vals))\n",
      "\n",
      "\n",
      "        return dice\n",
      "\n",
      "def dice_loss(y_true, y_pred):\n",
      "        return (1/binary_dice_coeff(y_true_y_pred)) - 1\n",
      "#testing customs metrics\n",
      "iou_metric = IoU_wrapper(2)\n",
      "dice_metric = dice_wrapper(2)\n",
      "\n",
      "model.compile(loss=dice_loss, optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
      "                                                                     binary_IoU_coef, binary_dice_coef])\n",
      "epochs=1\n",
      "history = model.fit(train_data_generator , \n",
      "                              validation_data=val_data_generator,\n",
      "                             epochs=epochs,\n",
      "                             verbose=1,)\n",
      "import keras.backend as K\n",
      "\n",
      "def IoU_wrapper(num_classes):\n",
      "    def IoU_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes Intersection-Over-Union also known as Jaccard Index\n",
      "        For multiclass task computes IoU individually for each class\n",
      "            and then returns average. Class labels should begin with 0\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float that is the average calculated IoU\n",
      "        '''\n",
      "\n",
      "        y_true = tf.math.round(y_true)\n",
      "        y_pred = tf.math.round(y_pred)\n",
      "        iou_array = tf.TensorArray(tf.float32, size=num_classes, clear_after_read=False)\n",
      "\n",
      "        for c in range(num_classes):\n",
      "            \n",
      "            y_true_vals = tf.cast(K.flatten(tf.math.equal(y_true, tf.constant(c, tf.float32))), tf.float32)\n",
      "            y_pred_vals = tf.cast(K.flatten(tf.math.equal(y_pred, tf.constant(c, tf.float32))), tf.float32)\n",
      "            intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "            union =  K.sum(y_true_vals) + K.sum(y_pred_vals) - intersection\n",
      "\n",
      "            iou_array = iou_array.write(c, intersection/union)\n",
      "        \n",
      "        return K.mean(iou_array.stack())\n",
      "    return IoU_coef\n",
      "\n",
      "\n",
      "def dice_wrapper(num_classes):\n",
      "    def dice_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes dice coefficient\n",
      "        For multiclass task computes dice individually for each class\n",
      "            and then returns average. Class labels should begin with 0\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        [a float that is the average dice]\n",
      "        '''\n",
      "        y_true = tf.math.round(y_true)\n",
      "        y_pred = tf.math.round(y_pred)\n",
      "        dice_array = tf.TensorArray(tf.float32, size=num_classes, clear_after_read=False)\n",
      "\n",
      "        for c in range(num_classes):\n",
      "            y_true_vals = tf.cast(K.flatten(tf.math.equal(y_true, tf.constant(c, tf.float32))), tf.float32)\n",
      "            y_pred_vals = tf.cast(K.flatten(tf.math.equal(y_pred, tf.constant(c, tf.float32))), tf.float32)\n",
      "            intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "\n",
      "            dice_array = dice_array.write(c, (2*intersection)/(K.sum(y_true_vals) + K.sum(y_pred_vals)))\n",
      "\n",
      "        \n",
      "        return K.mean(dice_array.stack())\n",
      "    return dice_coef\n",
      "\n",
      "\n",
      "def binary_IoU_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes Intersection-Over-Union also known as Jaccard Index\n",
      "        For binary segmentation\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float of calculated IoU\n",
      "        '''\n",
      "\n",
      "        y_true_vals = K.flatten(y_true)\n",
      "        y_pred_vals = K.flatten(y_pred)\n",
      "        intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "        union =  K.sum(y_true_vals) + K.sum(y_pred_vals) - intersection\n",
      "\n",
      "    \n",
      "        return intersection/union\n",
      "    \n",
      "def binary_dice_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes dice coefficient\n",
      "        For binary class segmentation\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float of dice coefficient]\n",
      "        '''\n",
      "\n",
      "        y_true_vals = K.flatten(y_true)\n",
      "        y_pred_vals = K.flatten(y_pred)\n",
      "        intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "\n",
      "        dice = (2*intersection)/(K.sum(y_true_vals) + K.sum(y_pred_vals))\n",
      "\n",
      "\n",
      "        return dice\n",
      "\n",
      "def dice_loss(y_true, y_pred):\n",
      "        return (1/binary_dice_coef(y_true_y_pred)) - 1\n",
      "#testing customs metrics\n",
      "iou_metric = IoU_wrapper(2)\n",
      "dice_metric = dice_wrapper(2)\n",
      "\n",
      "model.compile(loss=dice_loss, optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
      "                                                                     binary_IoU_coef, binary_dice_coef])\n",
      "epochs=1\n",
      "history = model.fit(train_data_generator , \n",
      "                              validation_data=val_data_generator,\n",
      "                             epochs=epochs,\n",
      "                             verbose=1,)\n",
      "import keras.backend as K\n",
      "\n",
      "def IoU_wrapper(num_classes):\n",
      "    def IoU_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes Intersection-Over-Union also known as Jaccard Index\n",
      "        For multiclass task computes IoU individually for each class\n",
      "            and then returns average. Class labels should begin with 0\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float that is the average calculated IoU\n",
      "        '''\n",
      "\n",
      "        y_true = tf.math.round(y_true)\n",
      "        y_pred = tf.math.round(y_pred)\n",
      "        iou_array = tf.TensorArray(tf.float32, size=num_classes, clear_after_read=False)\n",
      "\n",
      "        for c in range(num_classes):\n",
      "            \n",
      "            y_true_vals = tf.cast(K.flatten(tf.math.equal(y_true, tf.constant(c, tf.float32))), tf.float32)\n",
      "            y_pred_vals = tf.cast(K.flatten(tf.math.equal(y_pred, tf.constant(c, tf.float32))), tf.float32)\n",
      "            intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "            union =  K.sum(y_true_vals) + K.sum(y_pred_vals) - intersection\n",
      "\n",
      "            iou_array = iou_array.write(c, intersection/union)\n",
      "        \n",
      "        return K.mean(iou_array.stack())\n",
      "    return IoU_coef\n",
      "\n",
      "\n",
      "def dice_wrapper(num_classes):\n",
      "    def dice_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes dice coefficient\n",
      "        For multiclass task computes dice individually for each class\n",
      "            and then returns average. Class labels should begin with 0\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        [a float that is the average dice]\n",
      "        '''\n",
      "        y_true = tf.math.round(y_true)\n",
      "        y_pred = tf.math.round(y_pred)\n",
      "        dice_array = tf.TensorArray(tf.float32, size=num_classes, clear_after_read=False)\n",
      "\n",
      "        for c in range(num_classes):\n",
      "            y_true_vals = tf.cast(K.flatten(tf.math.equal(y_true, tf.constant(c, tf.float32))), tf.float32)\n",
      "            y_pred_vals = tf.cast(K.flatten(tf.math.equal(y_pred, tf.constant(c, tf.float32))), tf.float32)\n",
      "            intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "\n",
      "            dice_array = dice_array.write(c, (2*intersection)/(K.sum(y_true_vals) + K.sum(y_pred_vals)))\n",
      "\n",
      "        \n",
      "        return K.mean(dice_array.stack())\n",
      "    return dice_coef\n",
      "\n",
      "\n",
      "def binary_IoU_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes Intersection-Over-Union also known as Jaccard Index\n",
      "        For binary segmentation\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float of calculated IoU\n",
      "        '''\n",
      "\n",
      "        y_true_vals = K.flatten(y_true)\n",
      "        y_pred_vals = K.flatten(y_pred)\n",
      "        intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "        union =  K.sum(y_true_vals) + K.sum(y_pred_vals) - intersection\n",
      "\n",
      "    \n",
      "        return intersection/union\n",
      "    \n",
      "def binary_dice_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes dice coefficient\n",
      "        For binary class segmentation\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float of dice coefficient]\n",
      "        '''\n",
      "\n",
      "        y_true_vals = K.flatten(y_true)\n",
      "        y_pred_vals = K.flatten(y_pred)\n",
      "        intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "\n",
      "        dice = (2*intersection)/(K.sum(y_true_vals) + K.sum(y_pred_vals))\n",
      "\n",
      "\n",
      "        return dice\n",
      "\n",
      "def dice_loss(y_true, y_pred):\n",
      "        return (1/binary_dice_coef(y_true, y_pred)) - 1\n",
      "#testing customs metrics\n",
      "iou_metric = IoU_wrapper(2)\n",
      "dice_metric = dice_wrapper(2)\n",
      "\n",
      "model.compile(loss=dice_loss, optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
      "                                                                     binary_IoU_coef, binary_dice_coef])\n",
      "epochs=1\n",
      "history = model.fit(train_data_generator , \n",
      "                              validation_data=val_data_generator,\n",
      "                             epochs=epochs,\n",
      "                             verbose=1,)\n",
      "class TrainDataGenerator(keras.utils.Sequence):\n",
      "\n",
      "    def __init__(self, samples_list, y_dict, x_dict, batch_size=16, dim=(80,120,120), n_channels=4, shuffle=True):\n",
      "     \n",
      "        self.samples_list=samples_list\n",
      "        self.y_dict=y_dict\n",
      "        self.x_dict=x_dict\n",
      "        self.batch_size=batch_size\n",
      "        self.dim=dim\n",
      "        self.n_channels=n_channels\n",
      "        self.shuffle=shuffle\n",
      "        self.on_epoch_end()\n",
      "\n",
      "    def __len__(self):\n",
      "        #Take all batches in each iteration'\n",
      "        return int(np.floor(len(self.samples_list) / self.batch_size))\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        'Get next batch'\n",
      "        # Generate indexes of the batch\n",
      "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
      "\n",
      "        # single file\n",
      "        samples_list_temp = [samples_list[k] for k in indexes]\n",
      "\n",
      "        # Set of X_train and y_train\n",
      "        X, y = self.__data_generation(samples_list_temp)\n",
      "\n",
      "        return X, y\n",
      "\n",
      "    def on_epoch_end(self):\n",
      "        #Updates indexes after each epoch'\n",
      "        self.indexes = np.arange(len(self.samples_list))\n",
      "        #shuffle data if shuffle is true\n",
      "        if self.shuffle == True:\n",
      "            np.random.shuffle(self.indexes)\n",
      "            \n",
      "\n",
      "    def __data_generation(self, samples_list_temp):\n",
      "        #Generates data containing batch_size samples'\n",
      "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
      "        y = np.empty((self.batch_size, 80,120,120,1), dtype=np.float32)\n",
      "        \n",
      "        # Generate data\n",
      "        for i, file_key in enumerate(samples_list_temp):\n",
      "            # Store sample\n",
      "            X[i,] = nibabel.load(self.x_dict[file_key]).get_fdata()\n",
      "            \n",
      "            # Store target segmentation\n",
      "            y[i,] = nibabel.load(self.y_dict[file_key]).get_fdata().astype('uint8')\n",
      "\n",
      "        return X, y\n",
      "\n",
      "        \n",
      "# 75-20-5 train-validation-test split of data \n",
      "\n",
      "#FIXME\n",
      "#adjusted to partial sample list\n",
      "remain_samples, test_samples, _ , _ = train_test_split(samples_list, samples_list, test_size=0.05, random_state=42)\n",
      "\n",
      "train_samples, val_samples, _ , _ = train_test_split(remain_samples, remain_samples, test_size=0.21, random_state=42)\n",
      "\n",
      "#build data generators\n",
      "\n",
      "img_data_gen_args = {'batch_size':1,\n",
      "                         'dim':(80,120, 120), \n",
      "                         'n_channels':4,\n",
      "                         'shuffle':True}\n",
      "\n",
      "train_data_generator = TrainDataGenerator(samples_list = train_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "val_data_generator = TrainDataGenerator(samples_list = test_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "import keras.backend as K\n",
      "\n",
      "def IoU_wrapper(num_classes):\n",
      "    def IoU_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes Intersection-Over-Union also known as Jaccard Index\n",
      "        For multiclass task computes IoU individually for each class\n",
      "            and then returns average. Class labels should begin with 0\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float that is the average calculated IoU\n",
      "        '''\n",
      "\n",
      "        y_true = tf.math.round(y_true)\n",
      "        y_pred = tf.math.round(y_pred)\n",
      "        iou_array = tf.TensorArray(tf.float32, size=num_classes, clear_after_read=False)\n",
      "\n",
      "        for c in range(num_classes):\n",
      "            \n",
      "            y_true_vals = tf.cast(K.flatten(tf.math.equal(y_true, tf.constant(c, tf.float32))), tf.float32)\n",
      "            y_pred_vals = tf.cast(K.flatten(tf.math.equal(y_pred, tf.constant(c, tf.float32))), tf.float32)\n",
      "            intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "            union =  K.sum(y_true_vals) + K.sum(y_pred_vals) - intersection\n",
      "\n",
      "            iou_array = iou_array.write(c, intersection/union)\n",
      "        \n",
      "        return K.mean(iou_array.stack())\n",
      "    return IoU_coef\n",
      "\n",
      "\n",
      "def dice_wrapper(num_classes):\n",
      "    def dice_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes dice coefficient\n",
      "        For multiclass task computes dice individually for each class\n",
      "            and then returns average. Class labels should begin with 0\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        [a float that is the average dice]\n",
      "        '''\n",
      "        y_true = tf.math.round(y_true)\n",
      "        y_pred = tf.math.round(y_pred)\n",
      "        dice_array = tf.TensorArray(tf.float32, size=num_classes, clear_after_read=False)\n",
      "\n",
      "        for c in range(num_classes):\n",
      "            y_true_vals = tf.cast(K.flatten(tf.math.equal(y_true, tf.constant(c, tf.float32))), tf.float32)\n",
      "            y_pred_vals = tf.cast(K.flatten(tf.math.equal(y_pred, tf.constant(c, tf.float32))), tf.float32)\n",
      "            intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "\n",
      "            dice_array = dice_array.write(c, (2*intersection)/(K.sum(y_true_vals) + K.sum(y_pred_vals)))\n",
      "\n",
      "        \n",
      "        return K.mean(dice_array.stack())\n",
      "    return dice_coef\n",
      "\n",
      "\n",
      "def binary_IoU_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes Intersection-Over-Union also known as Jaccard Index\n",
      "        For binary segmentation\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float of calculated IoU\n",
      "        '''\n",
      "\n",
      "        y_true_vals = K.flatten(y_true)\n",
      "        y_pred_vals = K.flatten(y_pred)\n",
      "        intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "        union =  K.sum(y_true_vals) + K.sum(y_pred_vals) - intersection\n",
      "\n",
      "    \n",
      "        return intersection/union\n",
      "    \n",
      "def binary_dice_coef(y_true, y_pred) -> float:\n",
      "        '''\n",
      "        Computes dice coefficient\n",
      "        For binary class segmentation\n",
      "\n",
      "        Inputs:\n",
      "        ground_truth: a numpy array of the correct labels\n",
      "        y_preds: a numpy array of the predicted labels\n",
      "        num_classes: an int for the number of classes\n",
      "\n",
      "        Output:\n",
      "        a float of dice coefficient]\n",
      "        '''\n",
      "\n",
      "        y_true_vals = K.flatten(y_true)\n",
      "        y_pred_vals = K.flatten(y_pred)\n",
      "        intersection = K.sum(y_true_vals * y_pred_vals)\n",
      "\n",
      "        dice = (2*intersection)/(K.sum(y_true_vals) + K.sum(y_pred_vals))\n",
      "\n",
      "\n",
      "        return dice\n",
      "\n",
      "def dice_loss(y_true, y_pred):\n",
      "        return (1/binary_dice_coef(y_true, y_pred)) - 1\n",
      "#testing customs metrics\n",
      "iou_metric = IoU_wrapper(2)\n",
      "dice_metric = dice_wrapper(2)\n",
      "\n",
      "model.compile(loss=dice_loss, optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
      "                                                                     binary_IoU_coef, binary_dice_coef])\n",
      "epochs=1\n",
      "history = model.fit(train_data_generator , \n",
      "                              validation_data=val_data_generator,\n",
      "                             epochs=epochs,\n",
      "                             verbose=1,)\n",
      "\n",
      "checkpoint = keras.callbacks.ModelCheckpoint('resized_MRI_segmentation_dice_loss.h5', save_weights_only=True)\n",
      "early_stopping = keras.callbacks.EarlyStopping(monitor=binary_dice_coef, patience=10, mode='max')\n",
      "\n",
      "callbacks = [checkpoint, early_stopping]\n",
      "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
      "                                                                     binary_IoU_coef, binary_dice_coef])\n",
      "\n",
      "epochs=30\n",
      "history = model.fit(train_data_generator , \n",
      "                              validation_data=val_data_generator,\n",
      "                             epochs=epochs,\n",
      "                             verbose=1,\n",
      "                             callbacks=callbacks)\n",
      "\n",
      "checkpoint = keras.callbacks.ModelCheckpoint('resized_MRI_segmentation_dice_loss.h5', save_weights_only=True)\n",
      "early_stopping = keras.callbacks.EarlyStopping(monitor=binary_dice_coef, patience=10, mode='max')\n",
      "\n",
      "callbacks = [checkpoint, early_stopping]\n",
      "model.compile(loss=dice_loss, optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
      "                                                                     binary_IoU_coef, binary_dice_coef])\n",
      "\n",
      "epochs=30\n",
      "history = model.fit(train_data_generator , \n",
      "                              validation_data=val_data_generator,\n",
      "                             epochs=epochs,\n",
      "                             verbose=1,\n",
      "                             callbacks=callbacks)\n",
      "import os\n",
      "import numpy as np\n",
      "import nibabel\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "from tensorflow import keras\n",
      "import tensorflow as tf\n",
      "class TrainDataGenerator(keras.utils.Sequence):\n",
      "\n",
      "    def __init__(self, samples_list, y_dict, x_dict, batch_size=16, dim=(80,120,120), n_channels=4, shuffle=True):\n",
      "     \n",
      "        self.samples_list=samples_list\n",
      "        self.y_dict=y_dict\n",
      "        self.x_dict=x_dict\n",
      "        self.batch_size=batch_size\n",
      "        self.dim=dim\n",
      "        self.n_channels=n_channels\n",
      "        self.shuffle=shuffle\n",
      "        self.on_epoch_end()\n",
      "\n",
      "    def __len__(self):\n",
      "        #Take all batches in each iteration'\n",
      "        return int(np.floor(len(self.samples_list) / self.batch_size))\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        'Get next batch'\n",
      "        # Generate indexes of the batch\n",
      "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
      "\n",
      "        # single file\n",
      "        samples_list_temp = [samples_list[k] for k in indexes]\n",
      "\n",
      "        # Set of X_train and y_train\n",
      "        X, y = self.__data_generation(samples_list_temp)\n",
      "\n",
      "        return X, y\n",
      "\n",
      "    def on_epoch_end(self):\n",
      "        #Updates indexes after each epoch'\n",
      "        self.indexes = np.arange(len(self.samples_list))\n",
      "        #shuffle data if shuffle is true\n",
      "        if self.shuffle == True:\n",
      "            np.random.shuffle(self.indexes)\n",
      "            \n",
      "\n",
      "    def __data_generation(self, samples_list_temp):\n",
      "        #Generates data containing batch_size samples'\n",
      "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
      "        y = np.empty((self.batch_size, 80,120,120,1), dtype=np.float32)\n",
      "        \n",
      "        # Generate data\n",
      "        for i, file_key in enumerate(samples_list_temp):\n",
      "            # Store sample\n",
      "            X[i,] = tf.sparse.from_dense(nibabel.load(self.x_dict[file_key]).get_fdata())\n",
      "            \n",
      "            # Store target segmentation\n",
      "            y[i,] = tf.sparse.from_dense(nibabel.load(self.y_dict[file_key]).get_fdata())\n",
      "\n",
      "        return X, y\n",
      "\n",
      "        \n",
      "# 75-20-5 train-validation-test split of data \n",
      "\n",
      "#FIXME\n",
      "#adjusted to partial sample list\n",
      "remain_samples, test_samples, _ , _ = train_test_split(samples_list, samples_list, test_size=0.05, random_state=42)\n",
      "\n",
      "train_samples, val_samples, _ , _ = train_test_split(remain_samples, remain_samples, test_size=0.21, random_state=42)\n",
      "\n",
      "#build data generators\n",
      "\n",
      "img_data_gen_args = {'batch_size':1,\n",
      "                         'dim':(80,120, 120), \n",
      "                         'n_channels':4,\n",
      "                         'shuffle':True}\n",
      "\n",
      "train_data_generator = TrainDataGenerator(samples_list = train_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "val_data_generator = TrainDataGenerator(samples_list = test_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "checkpoint = keras.callbacks.ModelCheckpoint('resized_MRI_segmentation_dice_loss.h5', save_weights_only=True)\n",
      "early_stopping = keras.callbacks.EarlyStopping(monitor='binary_dice_coef', patience=10, mode='max')\n",
      "\n",
      "callbacks = [checkpoint, early_stopping]\n",
      "model.compile(loss=dice_loss, optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
      "                                                                     binary_IoU_coef, binary_dice_coef])\n",
      "\n",
      "epochs=30\n",
      "history = model.fit(train_data_generator , \n",
      "                              validation_data=val_data_generator,\n",
      "                             epochs=epochs,\n",
      "                             verbose=1,\n",
      "                             callbacks=callbacks)\n",
      "class TrainDataGenerator(keras.utils.Sequence):\n",
      "\n",
      "    def __init__(self, samples_list, y_dict, x_dict, batch_size=16, dim=(80,120,120), n_channels=4, shuffle=True):\n",
      "     \n",
      "        self.samples_list=samples_list\n",
      "        self.y_dict=y_dict\n",
      "        self.x_dict=x_dict\n",
      "        self.batch_size=batch_size\n",
      "        self.dim=dim\n",
      "        self.n_channels=n_channels\n",
      "        self.shuffle=shuffle\n",
      "        self.on_epoch_end()\n",
      "\n",
      "    def __len__(self):\n",
      "        #Take all batches in each iteration'\n",
      "        return int(np.floor(len(self.samples_list) / self.batch_size))\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        'Get next batch'\n",
      "        # Generate indexes of the batch\n",
      "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
      "\n",
      "        # single file\n",
      "        samples_list_temp = [samples_list[k] for k in indexes]\n",
      "\n",
      "        # Set of X_train and y_train\n",
      "        X, y = self.__data_generation(samples_list_temp)\n",
      "\n",
      "        return X, y\n",
      "\n",
      "    def on_epoch_end(self):\n",
      "        #Updates indexes after each epoch'\n",
      "        self.indexes = np.arange(len(self.samples_list))\n",
      "        #shuffle data if shuffle is true\n",
      "        if self.shuffle == True:\n",
      "            np.random.shuffle(self.indexes)\n",
      "            \n",
      "\n",
      "    def __data_generation(self, samples_list_temp):\n",
      "        #Generates data containing batch_size samples'\n",
      "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
      "        y = np.empty((self.batch_size, 80,120,120,1), dtype=np.float32)\n",
      "        \n",
      "        # Generate data\n",
      "        for i, file_key in enumerate(samples_list_temp):\n",
      "            # Store sample\n",
      "            X[i,] = nibabel.load(self.x_dict[file_key]).get_fdata()\n",
      "            \n",
      "            # Store target segmentation\n",
      "            y[i,] = nibabel.load(self.y_dict[file_key]).get_fdata()\n",
      "\n",
      "        return tf.sparse.from_dense(X), tf.sparse.from_dense(y)\n",
      "\n",
      "        \n",
      "# 75-20-5 train-validation-test split of data \n",
      "\n",
      "#FIXME\n",
      "#adjusted to partial sample list\n",
      "remain_samples, test_samples, _ , _ = train_test_split(samples_list, samples_list, test_size=0.05, random_state=42)\n",
      "\n",
      "train_samples, val_samples, _ , _ = train_test_split(remain_samples, remain_samples, test_size=0.21, random_state=42)\n",
      "\n",
      "#build data generators\n",
      "\n",
      "img_data_gen_args = {'batch_size':1,\n",
      "                         'dim':(80,120, 120), \n",
      "                         'n_channels':4,\n",
      "                         'shuffle':True}\n",
      "\n",
      "train_data_generator = TrainDataGenerator(samples_list = train_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "val_data_generator = TrainDataGenerator(samples_list = test_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "checkpoint = keras.callbacks.ModelCheckpoint('resized_MRI_segmentation_dice_loss.h5', save_weights_only=True)\n",
      "early_stopping = keras.callbacks.EarlyStopping(monitor='binary_dice_coef', patience=10, mode='max')\n",
      "\n",
      "callbacks = [checkpoint, early_stopping]\n",
      "model.compile(loss=dice_loss, optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
      "                                                                     binary_IoU_coef, binary_dice_coef])\n",
      "\n",
      "epochs=30\n",
      "history = model.fit(train_data_generator , \n",
      "                              validation_data=val_data_generator,\n",
      "                             epochs=epochs,\n",
      "                             verbose=1,\n",
      "                             callbacks=callbacks)\n",
      "class TrainDataGenerator(keras.utils.Sequence):\n",
      "\n",
      "    def __init__(self, samples_list, y_dict, x_dict, batch_size=16, dim=(80,120,120), n_channels=4, shuffle=True):\n",
      "     \n",
      "        self.samples_list=samples_list\n",
      "        self.y_dict=y_dict\n",
      "        self.x_dict=x_dict\n",
      "        self.batch_size=batch_size\n",
      "        self.dim=dim\n",
      "        self.n_channels=n_channels\n",
      "        self.shuffle=shuffle\n",
      "        self.on_epoch_end()\n",
      "\n",
      "    def __len__(self):\n",
      "        #Take all batches in each iteration'\n",
      "        return int(np.floor(len(self.samples_list) / self.batch_size))\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        'Get next batch'\n",
      "        # Generate indexes of the batch\n",
      "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
      "\n",
      "        # single file\n",
      "        samples_list_temp = [samples_list[k] for k in indexes]\n",
      "\n",
      "        # Set of X_train and y_train\n",
      "        X, y = self.__data_generation(samples_list_temp)\n",
      "\n",
      "        return X, y\n",
      "\n",
      "    def on_epoch_end(self):\n",
      "        #Updates indexes after each epoch'\n",
      "        self.indexes = np.arange(len(self.samples_list))\n",
      "        #shuffle data if shuffle is true\n",
      "        if self.shuffle == True:\n",
      "            np.random.shuffle(self.indexes)\n",
      "            \n",
      "\n",
      "    def __data_generation(self, samples_list_temp):\n",
      "        #Generates data containing batch_size samples'\n",
      "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
      "        y = np.empty((self.batch_size, 80,120,120,1), dtype=np.float32)\n",
      "        \n",
      "        # Generate data\n",
      "        for i, file_key in enumerate(samples_list_temp):\n",
      "            # Store sample\n",
      "            X[i,] = nibabel.load(self.x_dict[file_key]).get_fdata()\n",
      "            \n",
      "            # Store target segmentation\n",
      "            y[i,] = nibabel.load(self.y_dict[file_key]).get_fdata()\n",
      "\n",
      "        X = tf.sparse.from_dense(X)\n",
      "        y = tf.sparse.from_dense(y)\n",
      "        return tf.cast(X, tf.float32), tf.cast(y. tf.float32)\n",
      "\n",
      "        \n",
      "# 75-20-5 train-validation-test split of data \n",
      "\n",
      "#FIXME\n",
      "#adjusted to partial sample list\n",
      "remain_samples, test_samples, _ , _ = train_test_split(samples_list, samples_list, test_size=0.05, random_state=42)\n",
      "\n",
      "train_samples, val_samples, _ , _ = train_test_split(remain_samples, remain_samples, test_size=0.21, random_state=42)\n",
      "\n",
      "#build data generators\n",
      "\n",
      "img_data_gen_args = {'batch_size':1,\n",
      "                         'dim':(80,120, 120), \n",
      "                         'n_channels':4,\n",
      "                         'shuffle':True}\n",
      "\n",
      "train_data_generator = TrainDataGenerator(samples_list = train_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "val_data_generator = TrainDataGenerator(samples_list = test_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "checkpoint = keras.callbacks.ModelCheckpoint('resized_MRI_segmentation_dice_loss.h5', save_weights_only=True)\n",
      "early_stopping = keras.callbacks.EarlyStopping(monitor='binary_dice_coef', patience=10, mode='max')\n",
      "\n",
      "callbacks = [checkpoint, early_stopping]\n",
      "model.compile(loss=dice_loss, optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
      "                                                                     binary_IoU_coef, binary_dice_coef])\n",
      "\n",
      "epochs=30\n",
      "history = model.fit(train_data_generator , \n",
      "                              validation_data=val_data_generator,\n",
      "                             epochs=epochs,\n",
      "                             verbose=1,\n",
      "                             callbacks=callbacks)\n",
      "class TrainDataGenerator(keras.utils.Sequence):\n",
      "\n",
      "    def __init__(self, samples_list, y_dict, x_dict, batch_size=16, dim=(80,120,120), n_channels=4, shuffle=True):\n",
      "     \n",
      "        self.samples_list=samples_list\n",
      "        self.y_dict=y_dict\n",
      "        self.x_dict=x_dict\n",
      "        self.batch_size=batch_size\n",
      "        self.dim=dim\n",
      "        self.n_channels=n_channels\n",
      "        self.shuffle=shuffle\n",
      "        self.on_epoch_end()\n",
      "\n",
      "    def __len__(self):\n",
      "        #Take all batches in each iteration'\n",
      "        return int(np.floor(len(self.samples_list) / self.batch_size))\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        'Get next batch'\n",
      "        # Generate indexes of the batch\n",
      "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
      "\n",
      "        # single file\n",
      "        samples_list_temp = [samples_list[k] for k in indexes]\n",
      "\n",
      "        # Set of X_train and y_train\n",
      "        X, y = self.__data_generation(samples_list_temp)\n",
      "\n",
      "        return X, y\n",
      "\n",
      "    def on_epoch_end(self):\n",
      "        #Updates indexes after each epoch'\n",
      "        self.indexes = np.arange(len(self.samples_list))\n",
      "        #shuffle data if shuffle is true\n",
      "        if self.shuffle == True:\n",
      "            np.random.shuffle(self.indexes)\n",
      "            \n",
      "\n",
      "    def __data_generation(self, samples_list_temp):\n",
      "        #Generates data containing batch_size samples'\n",
      "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
      "        y = np.empty((self.batch_size, 80,120,120,1), dtype=np.float32)\n",
      "        \n",
      "        # Generate data\n",
      "        for i, file_key in enumerate(samples_list_temp):\n",
      "            # Store sample\n",
      "            X[i,] = nibabel.load(self.x_dict[file_key]).get_fdata()\n",
      "            \n",
      "            # Store target segmentation\n",
      "            y[i,] = nibabel.load(self.y_dict[file_key]).get_fdata()\n",
      "\n",
      "        X = tf.sparse.from_dense(X)\n",
      "        y = tf.sparse.from_dense(y)\n",
      "        return tf.cast(X, tf.float32), tf.cast(y, tf.float32)\n",
      "\n",
      "        \n",
      "# 75-20-5 train-validation-test split of data \n",
      "\n",
      "#FIXME\n",
      "#adjusted to partial sample list\n",
      "remain_samples, test_samples, _ , _ = train_test_split(samples_list, samples_list, test_size=0.05, random_state=42)\n",
      "\n",
      "train_samples, val_samples, _ , _ = train_test_split(remain_samples, remain_samples, test_size=0.21, random_state=42)\n",
      "\n",
      "#build data generators\n",
      "\n",
      "img_data_gen_args = {'batch_size':1,\n",
      "                         'dim':(80,120, 120), \n",
      "                         'n_channels':4,\n",
      "                         'shuffle':True}\n",
      "\n",
      "train_data_generator = TrainDataGenerator(samples_list = train_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "val_data_generator = TrainDataGenerator(samples_list = test_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "checkpoint = keras.callbacks.ModelCheckpoint('resized_MRI_segmentation_dice_loss.h5', save_weights_only=True)\n",
      "early_stopping = keras.callbacks.EarlyStopping(monitor='binary_dice_coef', patience=10, mode='max')\n",
      "\n",
      "callbacks = [checkpoint, early_stopping]\n",
      "model.compile(loss=dice_loss, optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
      "                                                                     binary_IoU_coef, binary_dice_coef])\n",
      "\n",
      "epochs=30\n",
      "history = model.fit(train_data_generator , \n",
      "                              validation_data=val_data_generator,\n",
      "                             epochs=epochs,\n",
      "                             verbose=1,\n",
      "                             callbacks=callbacks)\n",
      "class TrainDataGenerator(keras.utils.Sequence):\n",
      "\n",
      "    def __init__(self, samples_list, y_dict, x_dict, batch_size=16, dim=(80,120,120), n_channels=4, shuffle=True):\n",
      "     \n",
      "        self.samples_list=samples_list\n",
      "        self.y_dict=y_dict\n",
      "        self.x_dict=x_dict\n",
      "        self.batch_size=batch_size\n",
      "        self.dim=dim\n",
      "        self.n_channels=n_channels\n",
      "        self.shuffle=shuffle\n",
      "        self.on_epoch_end()\n",
      "\n",
      "    def __len__(self):\n",
      "        #Take all batches in each iteration'\n",
      "        return int(np.floor(len(self.samples_list) / self.batch_size))\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        'Get next batch'\n",
      "        # Generate indexes of the batch\n",
      "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
      "\n",
      "        # single file\n",
      "        samples_list_temp = [samples_list[k] for k in indexes]\n",
      "\n",
      "        # Set of X_train and y_train\n",
      "        X, y = self.__data_generation(samples_list_temp)\n",
      "\n",
      "        return X, y\n",
      "\n",
      "    def on_epoch_end(self):\n",
      "        #Updates indexes after each epoch'\n",
      "        self.indexes = np.arange(len(self.samples_list))\n",
      "        #shuffle data if shuffle is true\n",
      "        if self.shuffle == True:\n",
      "            np.random.shuffle(self.indexes)\n",
      "            \n",
      "\n",
      "    def __data_generation(self, samples_list_temp):\n",
      "        #Generates data containing batch_size samples'\n",
      "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
      "        y = np.empty((self.batch_size, 80,120,120,1), dtype=np.float32)\n",
      "        \n",
      "        # Generate data\n",
      "        for i, file_key in enumerate(samples_list_temp):\n",
      "            # Store sample\n",
      "            X[i,] = nibabel.load(self.x_dict[file_key]).get_fdata()\n",
      "            \n",
      "            # Store target segmentation\n",
      "            y[i,] = nibabel.load(self.y_dict[file_key]).get_fdata()\n",
      "\n",
      "        return X, y\n",
      "\n",
      "        \n",
      "# 75-20-5 train-validation-test split of data \n",
      "\n",
      "#FIXME\n",
      "#adjusted to partial sample list\n",
      "remain_samples, test_samples, _ , _ = train_test_split(samples_list, samples_list, test_size=0.05, random_state=42)\n",
      "\n",
      "train_samples, val_samples, _ , _ = train_test_split(remain_samples, remain_samples, test_size=0.21, random_state=42)\n",
      "\n",
      "#build data generators\n",
      "\n",
      "img_data_gen_args = {'batch_size':1,\n",
      "                         'dim':(80,120, 120), \n",
      "                         'n_channels':4,\n",
      "                         'shuffle':True}\n",
      "\n",
      "train_data_generator = TrainDataGenerator(samples_list = train_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "val_data_generator = TrainDataGenerator(samples_list = test_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "class TrainDataGenerator(keras.utils.Sequence):\n",
      "\n",
      "    def __init__(self, samples_list, y_dict, x_dict, batch_size=16, dim=(80,120,120), n_channels=4, shuffle=True):\n",
      "     \n",
      "        self.samples_list=samples_list\n",
      "        self.y_dict=y_dict\n",
      "        self.x_dict=x_dict\n",
      "        self.batch_size=batch_size\n",
      "        self.dim=dim\n",
      "        self.n_channels=n_channels\n",
      "        self.shuffle=shuffle\n",
      "        self.on_epoch_end()\n",
      "\n",
      "    def __len__(self):\n",
      "        #Take all batches in each iteration'\n",
      "        return int(np.floor(len(self.samples_list) / self.batch_size))\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        'Get next batch'\n",
      "        # Generate indexes of the batch\n",
      "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
      "\n",
      "        # single file\n",
      "        samples_list_temp = [samples_list[k] for k in indexes]\n",
      "\n",
      "        # Set of X_train and y_train\n",
      "        X, y = self.__data_generation(samples_list_temp)\n",
      "\n",
      "        return X, y\n",
      "\n",
      "    def on_epoch_end(self):\n",
      "        #Updates indexes after each epoch'\n",
      "        self.indexes = np.arange(len(self.samples_list))\n",
      "        #shuffle data if shuffle is true\n",
      "        if self.shuffle == True:\n",
      "            np.random.shuffle(self.indexes)\n",
      "            \n",
      "\n",
      "    def __data_generation(self, samples_list_temp):\n",
      "        #Generates data containing batch_size samples'\n",
      "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
      "        y = np.empty((self.batch_size, 80,120,120,1), dtype=np.float32)\n",
      "        \n",
      "        # Generate data\n",
      "        for i, file_key in enumerate(samples_list_temp):\n",
      "            # Store sample\n",
      "            X[i,] = nibabel.load(self.x_dict[file_key]).get_fdata()\n",
      "            \n",
      "            # Store target segmentation\n",
      "            y[i,] = nibabel.load(self.y_dict[file_key]).get_fdata()\n",
      "\n",
      "        return X, y\n",
      "\n",
      "        \n",
      "# 75-20-5 train-validation-test split of data \n",
      "\n",
      "#FIXME\n",
      "#adjusted to partial sample list\n",
      "remain_samples, test_samples, _ , _ = train_test_split(samples_list, samples_list, test_size=0.05, random_state=42)\n",
      "\n",
      "train_samples, val_samples, _ , _ = train_test_split(remain_samples, remain_samples, test_size=0.21, random_state=42)\n",
      "\n",
      "#build data generators\n",
      "\n",
      "img_data_gen_args = {'batch_size':2,\n",
      "                         'dim':(80,120, 120), \n",
      "                         'n_channels':4,\n",
      "                         'shuffle':True}\n",
      "\n",
      "train_data_generator = TrainDataGenerator(samples_list = train_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "val_data_generator = TrainDataGenerator(samples_list = test_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "checkpoint = keras.callbacks.ModelCheckpoint('resized_MRI_segmentation_dice_loss.h5', save_weights_only=True)\n",
      "early_stopping = keras.callbacks.EarlyStopping(monitor='binary_dice_coef', patience=10, mode='max')\n",
      "\n",
      "callbacks = [checkpoint, early_stopping]\n",
      "model.compile(loss=dice_loss, optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
      "                                                                     binary_IoU_coef, binary_dice_coef])\n",
      "\n",
      "epochs=30\n",
      "history = model.fit(train_data_generator , \n",
      "                              validation_data=val_data_generator,\n",
      "                             epochs=epochs,\n",
      "                             verbose=1,\n",
      "                             callbacks=callbacks)\n",
      "class TrainDataGenerator(keras.utils.Sequence):\n",
      "\n",
      "    def __init__(self, samples_list, y_dict, x_dict, batch_size=16, dim=(80,120,120), n_channels=4, shuffle=True):\n",
      "     \n",
      "        self.samples_list=samples_list\n",
      "        self.y_dict=y_dict\n",
      "        self.x_dict=x_dict\n",
      "        self.batch_size=batch_size\n",
      "        self.dim=dim\n",
      "        self.n_channels=n_channels\n",
      "        self.shuffle=shuffle\n",
      "        self.on_epoch_end()\n",
      "\n",
      "    def __len__(self):\n",
      "        #Take all batches in each iteration'\n",
      "        return int(np.floor(len(self.samples_list) / self.batch_size))\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        'Get next batch'\n",
      "        # Generate indexes of the batch\n",
      "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
      "\n",
      "        # single file\n",
      "        samples_list_temp = [samples_list[k] for k in indexes]\n",
      "\n",
      "        # Set of X_train and y_train\n",
      "        X, y = self.__data_generation(samples_list_temp)\n",
      "\n",
      "        return X, y\n",
      "\n",
      "    def on_epoch_end(self):\n",
      "        #Updates indexes after each epoch'\n",
      "        self.indexes = np.arange(len(self.samples_list))\n",
      "        #shuffle data if shuffle is true\n",
      "        if self.shuffle == True:\n",
      "            np.random.shuffle(self.indexes)\n",
      "            \n",
      "\n",
      "    def __data_generation(self, samples_list_temp):\n",
      "        #Generates data containing batch_size samples'\n",
      "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
      "        y = np.empty((self.batch_size, 80,120,120,1), dtype=np.float32)\n",
      "        \n",
      "        # Generate data\n",
      "        for i, file_key in enumerate(samples_list_temp):\n",
      "            # Store sample\n",
      "            X[i,] = nibabel.load(self.x_dict[file_key]).get_fdata()\n",
      "            \n",
      "            # Store target segmentation\n",
      "            y[i,] = nibabel.load(self.y_dict[file_key]).get_fdata()\n",
      "\n",
      "        return X, y\n",
      "\n",
      "        \n",
      "# 75-20-5 train-validation-test split of data \n",
      "\n",
      "#FIXME\n",
      "#adjusted to partial sample list\n",
      "remain_samples, test_samples, _ , _ = train_test_split(samples_list, samples_list, test_size=0.05, random_state=42)\n",
      "\n",
      "train_samples, val_samples, _ , _ = train_test_split(remain_samples, remain_samples, test_size=0.21, random_state=42)\n",
      "\n",
      "#build data generators\n",
      "\n",
      "img_data_gen_args = {'batch_size':4,\n",
      "                         'dim':(80,120, 120), \n",
      "                         'n_channels':4,\n",
      "                         'shuffle':True}\n",
      "\n",
      "train_data_generator = TrainDataGenerator(samples_list = train_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "val_data_generator = TrainDataGenerator(samples_list = test_samples,\n",
      "                                          y_dict = seg_files,\n",
      "                                          x_dict = mri_files,\n",
      "                                          **img_data_gen_args)\n",
      "\n",
      "checkpoint = keras.callbacks.ModelCheckpoint('resized_MRI_segmentation_dice_loss.h5', save_weights_only=True)\n",
      "early_stopping = keras.callbacks.EarlyStopping(monitor='binary_dice_coef', patience=10, mode='max')\n",
      "\n",
      "callbacks = [checkpoint, early_stopping]\n",
      "model.compile(loss=dice_loss, optimizer=Adam(), metrics=['acc', iou_metric, dice_metric, \n",
      "                                                                     binary_IoU_coef, binary_dice_coef])\n",
      "\n",
      "epochs=30\n",
      "history = model.fit(train_data_generator , \n",
      "                              validation_data=val_data_generator,\n",
      "                             epochs=epochs,\n",
      "                             verbose=1,\n",
      "                             callbacks=callbacks)\n",
      "import pickle\n",
      "with open('/trainHistoryDict', 'wb') as file_pi:\n",
      "        pickle.dump(history.history, file_pi)\n",
      "with open('trainHistoryDict', 'wb') as file_pi:\n",
      "        pickle.dump(history.history, file_pi)\n",
      "history\n"
     ]
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee68e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
